{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections, csv, math, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figs 3-16 and 18-31.\n",
    "Input: LexiconHurvitzDataset.csv\n",
    "Output: early_late_freqs_per_book.csv and relative_per_book.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_list = []\n",
    "\n",
    "late_books = ['ezek', 'hag', 'zech', 'mal', 'song', 'qoh', 'esth', 'dan', 'ezra', 'neh', 'chr', '1chr', '2chr']\n",
    "core_late_books = ['1chr', '2chr', 'esth', 'dan', 'ezra', 'neh']\n",
    "chron = ['1chr', '2chr']\n",
    "synoptic = ['ChrS', 'ChrN']\n",
    "\n",
    "The information we need is collected first in several dictionaries \n",
    "\n",
    "#early_freqs and late_freqs collect the frequency of early and late variants per book\n",
    "early_freqs = collections.defaultdict(lambda:collections.Counter())\n",
    "late_freqs = collections.defaultdict(lambda:collections.Counter())\n",
    "\n",
    "#synop_early_freqs and synop_late_freqs collect the frequency of early and late variants in synoptic and non-synoptic Chronicles.\n",
    "synop_late_freqs = collections.defaultdict(lambda:collections.Counter())\n",
    "synop_early_freqs = collections.defaultdict(lambda:collections.Counter())\n",
    "\n",
    "#relative_dict and relative_synop collect the proportion of each variable occurring in a book or in synoptic/non-synoptic Chronicles. \n",
    "relative_synop = collections.defaultdict(lambda:collections.Counter())\n",
    "relative_dict = collections.defaultdict(lambda:collections.Counter())\n",
    "core_late_books_relative_dict = collections.defaultdict(float)\n",
    "core_late_books_late_dict = collections.defaultdict(int)\n",
    "core_late_books_early_dict = collections.defaultdict(int)\n",
    "\n",
    "with open('LexiconHurvitzDataset.csv', 'rt') as f:    \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        #first collect data for synoptic and non-synoptic Chronicles\n",
    "        if row[3] == '1':\n",
    "                if not row[1] == '7':\n",
    "                    synop_late_freqs[row[8]][row[1]] += int(row[7])\n",
    "        elif row[3] == '0':\n",
    "            if not row[1] == '7':\n",
    "                synop_early_freqs[row[8]][row[1]] += int(row[7])\n",
    "        #now the rest, select late variants\n",
    "        if row[3] == '1':\n",
    "            if not row[1] in feature_list:\n",
    "                feature_list.append(row[1])\n",
    "            if not row[1] == '7':\n",
    "                if row[4] in core_late_books:\n",
    "                    core_late_books_late_dict[row[1]] += int(row[7])\n",
    "                if row[4] in chron:\n",
    "                   late_freqs['chr'][row[1]] += int(row[7]) \n",
    "                else:\n",
    "                    late_freqs[row[4]][row[1]] += int(row[7])\n",
    "        #select early variants\n",
    "        elif row[3] == '0':\n",
    "            if not row[1] == '7':\n",
    "                if row[4] in core_late_books:\n",
    "                    core_late_books_early_dict[row[1]] += int(row[7])\n",
    "                if row[4] in chron:\n",
    "                   early_freqs['chr'][row[1]] += int(row[7])\n",
    "                else:    \n",
    "                    early_freqs[row[4]][row[1]] += int(row[7])\n",
    "\n",
    "#calculate the late fractions\n",
    "for book in late_books:\n",
    "    for feat in feature_list:\n",
    "        if late_freqs[book][feat] + early_freqs[book][feat] > 0:\n",
    "            relative_dict[book][feat] = late_freqs[book][feat]/(late_freqs[book][feat] + early_freqs[book][feat])\n",
    "\n",
    "for syn in synoptic:\n",
    "    for feat in feature_list:\n",
    "        if synop_late_freqs[syn][feat] + synop_early_freqs[syn][feat] > 0:\n",
    "            relative_synop[syn][feat] = synop_late_freqs[syn][feat]/ (synop_late_freqs[syn][feat] + synop_early_freqs[syn][feat])\n",
    "            \n",
    "for feat in feature_list:\n",
    "    if (core_late_books_late_dict[feat] + core_late_books_early_dict[feat]) > 0:\n",
    "        core_late_books_relative_dict[feat] = core_late_books_late_dict[feat]/ (core_late_books_late_dict[feat] + core_late_books_early_dict[feat])\n",
    "\n",
    "#store all the information in a csv\n",
    "csvh = open(r\"early_late_freqs_per_book.csv\", \"w\")\n",
    "row = ['book', 'feature', 'early_late', 'frequency']\n",
    "csvh.write('{}\\n'.format(','.join(row)))\n",
    "for book in late_books:\n",
    "    for feat in feature_list:\n",
    "        if not feat == '7':\n",
    "            row = []\n",
    "            row.append(book)\n",
    "            row.append(feat)\n",
    "            row.append('1')\n",
    "            if feat in late_freqs[book].keys():\n",
    "                row.append(str(late_freqs[book][feat]))\n",
    "            else:\n",
    "                row.append('0')\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "            row = []\n",
    "            row.append(book)\n",
    "            row.append(feat)\n",
    "            row.append('0')\n",
    "            if feat in early_freqs[book].keys():\n",
    "                row.append(str(-early_freqs[book][feat]))\n",
    "            else:\n",
    "                row.append('0')\n",
    "            csvh.write('{}\\n'.format(','.join(row))) \n",
    "for syn in synoptic:\n",
    "    for feat in feature_list:\n",
    "        if not feat == '7':\n",
    "            row = []\n",
    "            row.append(syn)\n",
    "            row.append(feat)\n",
    "            row.append('1')\n",
    "            if feat in synop_late_freqs[syn].keys():\n",
    "                row.append(str(synop_late_freqs[syn][feat]))\n",
    "            else:\n",
    "                row.append('0')\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "            row = []\n",
    "            row.append(syn)\n",
    "            row.append(feat)\n",
    "            row.append('0') \n",
    "            if feat in synop_early_freqs[syn].keys():\n",
    "                row.append(str(-synop_early_freqs[syn][feat]))\n",
    "            else:\n",
    "                row.append('0')\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "for feat in feature_list:\n",
    "        if not feat == '7':\n",
    "            row = []\n",
    "            row.append('core_late_books')\n",
    "            row.append(feat)\n",
    "            row.append('1')\n",
    "            if feat in core_late_books_late_dict.keys():\n",
    "                row.append(str(core_late_books_late_dict[feat]))\n",
    "            else:\n",
    "                row.append('0')\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "            row = []\n",
    "            row.append('core_late_books')\n",
    "            row.append(feat)\n",
    "            row.append('0')\n",
    "            if feat in core_late_books_early_dict.keys():\n",
    "                row.append(str(-core_late_books_early_dict[feat]))\n",
    "            else:\n",
    "                row.append('0')\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "            \n",
    "csvh.close()\n",
    "\n",
    "csvh = open(r\"relative_per_book.csv\", \"w\")\n",
    "row = ['book', 'feature', 'frequency']\n",
    "csvh.write('{}\\n'.format(','.join(row)))\n",
    "for book in late_books:\n",
    "    for fe in list(relative_dict[book].keys()):\n",
    "        if not fe == '7':\n",
    "            row = []\n",
    "            row.append(book)\n",
    "            row.append(fe)\n",
    "            row.append(str(relative_dict[book][fe]))\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "for syn in synoptic:\n",
    "    for fe in relative_synop[syn].keys():\n",
    "        if not fe == '7':\n",
    "            row = []\n",
    "            row.append(syn)\n",
    "            row.append(fe)\n",
    "            row.append(str(relative_synop[syn][fe]))\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "for fe in list(core_late_books_relative_dict.keys()):\n",
    "        if not fe == '7':\n",
    "            row = []\n",
    "            row.append('core_late_books')\n",
    "            row.append(fe)\n",
    "            row.append(str(core_late_books_relative_dict[fe]))\n",
    "            csvh.write('{}\\n'.format(','.join(row)))\n",
    "            \n",
    "csvh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For figure 17 we need the length of the books. We calculate these lengths using Laf Fabric. For more information about Laf Fabric, see shebanq.ancient-data.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s This is LAF-Fabric 4.5.23\n",
      "API reference: http://laf-fabric.readthedocs.org/en/latest/texts/API-reference.html\n",
      "Feature doc: https://shebanq.ancient-data.org/static/docs/featuredoc/texts/welcome.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "import laf\n",
    "from laf.fabric import LafFabric\n",
    "from etcbc.preprocess import prepare\n",
    "from etcbc.mql import MQL\n",
    "fabric = LafFabric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0.00s LOADING API: please wait ... \n",
      "  0.05s DETAIL: COMPILING m: UP TO DATE\n",
      "  0.13s USING main  DATA COMPILED AT: 2015-11-02T15-08-56\n",
      "  0.13s DETAIL: COMPILING a: UP TO DATE\n",
      "  0.21s DETAIL: load main: G.node_anchor_min\n",
      "  0.31s DETAIL: load main: G.node_anchor_max\n",
      "  0.40s DETAIL: load main: G.node_sort\n",
      "  0.50s DETAIL: load main: G.node_sort_inv\n",
      "  0.96s DETAIL: load main: G.edges_from\n",
      "  1.08s DETAIL: load main: G.edges_to\n",
      "  1.21s DETAIL: load main: F.etcbc4_db_monads [node] \n",
      "  1.96s DETAIL: load main: F.etcbc4_db_oid [node] \n",
      "  2.70s DETAIL: load main: F.etcbc4_db_otype [node] \n",
      "  3.37s DETAIL: load main: F.etcbc4_ft_det [node] \n",
      "  3.60s DETAIL: load main: F.etcbc4_ft_function [node] \n",
      "  3.72s DETAIL: load main: F.etcbc4_ft_g_word [node] \n",
      "  4.02s DETAIL: load main: F.etcbc4_ft_gn [node] \n",
      "  4.15s DETAIL: load main: F.etcbc4_ft_language [node] \n",
      "  4.35s DETAIL: load main: F.etcbc4_ft_lex [node] \n",
      "  4.55s DETAIL: load main: F.etcbc4_ft_nu [node] \n",
      "  4.72s DETAIL: load main: F.etcbc4_ft_number [node] \n",
      "  5.22s DETAIL: load main: F.etcbc4_ft_ps [node] \n",
      "  5.45s DETAIL: load main: F.etcbc4_ft_sp [node] \n",
      "  5.63s DETAIL: load main: F.etcbc4_ft_typ [node] \n",
      "  5.96s DETAIL: load main: F.etcbc4_ft_vs [node] \n",
      "  6.15s DETAIL: load main: F.etcbc4_ft_vt [node] \n",
      "  6.35s DETAIL: load main: F.etcbc4_sft_book [node] \n",
      "  6.37s DETAIL: load main: F.etcbc4_sft_chapter [node] \n",
      "  6.40s DETAIL: load main: F.etcbc4_sft_label [node] \n",
      "  6.44s DETAIL: load main: F.etcbc4_sft_verse [node] \n",
      "  6.47s LOGFILE=c:/Users/Martijn/laf-fabric-output/etcbc4b/mql/__log__mql.txt\n",
      "  6.47s INFO: LOADING PREPARED data: please wait ... \n",
      "  6.47s prep prep: G.node_sort\n",
      "  6.58s prep prep: G.node_sort_inv\n",
      "  7.11s prep prep: L.node_up\n",
      "  9.67s prep prep: L.node_down\n",
      "    15s prep prep: V.verses\n",
      "    15s prep prep: V.books_la\n",
      "    15s ETCBC reference: http://laf-fabric.readthedocs.org/en/latest/texts/ETCBC-reference.html\n",
      "    17s INFO: LOADED PREPARED data\n",
      "    17s INFO: DATA LOADED FROM SOURCE etcbc4b AND ANNOX lexicon FOR TASK mql AT 2016-05-13T07-21-06\n"
     ]
    }
   ],
   "source": [
    "API = fabric.load('etcbc4b', '--', 'mql', {\n",
    "    \"xmlids\": {\"node\": False, \"edge\": False},\n",
    "    \"features\": ('''\n",
    "        oid otype monads number\n",
    "        language lex g_word\n",
    "        sp gn nu ps vt vs\n",
    "        function typ det\n",
    "        book chapter verse label\n",
    "    ''',''),\n",
    "    \"prepare\": prepare,\n",
    "    \"primary\": False,\n",
    "}, verbose='DEBUG')\n",
    "exec(fabric.localnames.format(var='fabric'))\n",
    "Q = MQL(API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With the following MQL query all words in the MT are collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query1 = '''\n",
    "select all objects where\n",
    "[book [chapter [verse\n",
    "    [word language = Hebrew]\n",
    "]]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sheaf1 = Q.mql(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Booklengths are collected in book_length_dict_2, in which the booknames are converted in a shorter format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book_names = {'Genesis': 'gen', 'Exodus': 'exod', 'Leviticus': 'lev', 'Numeri': 'num', 'Deuteronomium': 'deut', 'Josua': 'josh',\n",
    "              'Judices': 'judg', 'Samuel_I': '1sam', 'Samuel_II': '2sam', 'Reges_I': '1kgs', 'Reges_II': '2kgs', 'Jesaia': 'isa',\n",
    "              'Jeremia': 'jer', 'Ezechiel': 'ezek', 'Hosea': 'hos', 'Joel': 'joel', 'Amos': 'amos', 'Obadia': 'oba', 'Jona': 'jona', \n",
    "              'Micha': 'mic', 'Nahum': 'nah', 'Habakuk': 'hab', 'Zephania': 'zeph', 'Haggai': 'hag', 'Sacharia': 'zech',\n",
    "              'Maleachi': 'mal', 'Psalmi': 'ps', 'Iob': 'job', 'Proverbia': 'prov', 'Ruth': 'ruth', 'Canticum': 'song',\n",
    "              'Ecclesiastes': 'qoh', 'Threni': 'lam', 'Esther': 'esth', 'Daniel': 'dan', 'Esra': 'ezra', 'Nehemia': 'neh',\n",
    "              'Chronica_I': '1chr', 'Chronica_II': '2chr'}\n",
    "book_length_dict_1 = collections.defaultdict(int)\n",
    "book_length_dict_2 = {}\n",
    "\n",
    "\n",
    "for ((bo, ((ch, ((ve, ((wo,),)),)),)),) in sheaf1.results():\n",
    "    book_length_dict_1[F.book.v(bo)] += 1\n",
    "\n",
    "for book in book_length_dict_1.keys():\n",
    "    book_length_dict_2[book_names[book]] = book_length_dict_1[book]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counts the number of overlapping items per book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "overlap_dict = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "double_items_dict = collections.defaultdict(int)\n",
    "\n",
    "all_overlap = ['1', '46', '53', '63', '3', '26', '11', '14', '17', '12', '22', '23', '13', '16', '45', '69', '48', '49', '51', '57', '66', '60', '70']\n",
    "overlap_groups = [['1', '46', '53', '63'], ['3', '26'], ['11', '14', '17'], ['12', '22', '23'], ['13', '16'], ['45', '69'], ['48', '49'], ['51', '57', '66'], ['60', '70']]\n",
    "\n",
    "with open('LexiconHurvitzDataset.csv', 'rt') as f:    \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[3] == '0':\n",
    "            if not row[4] in double_items_dict.keys():\n",
    "                if not row[4] == '-':\n",
    "                    double_items_dict[row[4]] = 0\n",
    "            if row[1] in all_overlap:\n",
    "                for feat_list in overlap_groups:\n",
    "                    if row[1] in feat_list:\n",
    "                        #for feature in len(feat_list):\n",
    "                        overlap_dict[(row[4], row[5], row[6])][tuple(feat_list)] = [0] * len(feat_list)\n",
    "                        \n",
    "with open('LexiconHurvitzDataset.csv', 'rt') as f:    \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[3] == '0':\n",
    "            if row[1] in all_overlap:\n",
    "                for feat_list in overlap_groups:\n",
    "                    if row[1] in feat_list:\n",
    "                        fea_list = overlap_dict[(row[4], row[5], row[6])][tuple(feat_list)]\n",
    "                        index = feat_list.index(row[1])\n",
    "                        fea_list[index] += int(row[7])\n",
    "                        \n",
    "for key in overlap_dict.keys():\n",
    "    for lis in overlap_dict[key].keys():\n",
    "        n_items = 0\n",
    "        for item in overlap_dict[key][lis]:\n",
    "            if item != 0:\n",
    "                n_items += 1\n",
    "        if n_items > 0:\n",
    "            double = sum(overlap_dict[key][lis]) - max(overlap_dict[key][lis])\n",
    "            double_items_dict[key[0]] += double  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: early_late_per_book.csv \n",
    "For each of the TBH/LBH books it stores the booklength,\n",
    "the absolute number of early and late types and tokens and the number of early and late types and tokens per 1000 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "late_books = ['ezek', 'hag', 'zech', 'mal', 'song', 'qoh', 'esth', 'dan', 'ezra', 'neh', 'chr']\n",
    "col_names = ['book', 'early_late', 'book_length', 'types', 'tokens', 'types_per_1000', 'tokens_per_1000']\n",
    "\n",
    "csvh = open(r\"early_late_per_book.csv\", \"w\")\n",
    "\n",
    "row= []\n",
    "for name in col_names:\n",
    "    row.append(name)\n",
    "csvh.write('{}\\n'.format(','.join(row)))\n",
    "\n",
    "for book in late_books:\n",
    "    if book == 'chr':\n",
    "        row = []\n",
    "        row.append(book)\n",
    "        row.append('early')\n",
    "        row.append(str(book_length_dict_2['1chr'] + book_length_dict_2['2chr']))\n",
    "        row.append(str(len(early_freqs[book])))\n",
    "        early_tokens = 0\n",
    "        for key in early_freqs[book].keys():\n",
    "            early_tokens += early_freqs[book][key]\n",
    "        #now we substract the number of overlapping items\n",
    "        early_tokens -= double_items_dict['1chr']\n",
    "        early_tokens -= double_items_dict['2chr']\n",
    "        row.append(str(early_tokens))\n",
    "        row.append(str((len(early_freqs[book]) * 1000 / (book_length_dict_2['1chr'] + book_length_dict_2['2chr']))))\n",
    "        row.append(str(-early_tokens * 1000 / (book_length_dict_2['1chr'] + book_length_dict_2['2chr'])))\n",
    "\n",
    "        csvh.write('{}\\n'.format(','.join(row)))\n",
    "    else:\n",
    "       row = []\n",
    "       row.append(book)\n",
    "       row.append('early')\n",
    "       row.append(str(book_length_dict_2[book]))\n",
    "       row.append(str(len(early_freqs[book])))\n",
    "       early_tokens = 0\n",
    "       for key in early_freqs[book].keys():\n",
    "           early_tokens += early_freqs[book][key]\n",
    "       early_tokens -= double_items_dict[book]\n",
    "       row.append(str(early_tokens))\n",
    "       row.append(str((len(early_freqs[book]) * 1000 / book_length_dict_2[book])))\n",
    "       row.append(str(-early_tokens * 1000 / book_length_dict_2[book]))\n",
    "\n",
    "       csvh.write('{}\\n'.format(','.join(row))) \n",
    "for book in late_books:\n",
    "    if book == 'chr':\n",
    "        row = []\n",
    "        row.append(book)\n",
    "        row.append('late')\n",
    "        row.append(str(book_length_dict_2['1chr'] + book_length_dict_2['2chr']))\n",
    "        row.append(str(len(late_freqs[book])))\n",
    "        late_tokens = 0\n",
    "        for key in late_freqs[book].keys():\n",
    "            late_tokens += late_freqs[book][key]\n",
    "        row.append(str(late_tokens))\n",
    "        row.append(str((len(late_freqs[book]) * 1000 / (book_length_dict_2['1chr'] + book_length_dict_2['2chr']))))\n",
    "        row.append(str(late_tokens * 1000 / (book_length_dict_2['1chr'] + book_length_dict_2['2chr'])))\n",
    "        \n",
    "        csvh.write('{}\\n'.format(','.join(row)))\n",
    "    else:\n",
    "       row = []\n",
    "       row.append(book)\n",
    "       row.append('late')\n",
    "       row.append(str(book_length_dict_2[book]))\n",
    "       row.append(str(len(late_freqs[book])))\n",
    "       late_tokens = 0\n",
    "       for key in late_freqs[book].keys():\n",
    "           late_tokens += late_freqs[book][key]\n",
    "       row.append(str(late_tokens))\n",
    "       row.append(str((len(late_freqs[book]) * 1000 / book_length_dict_2[book])))\n",
    "       row.append(str(late_tokens * 1000 / book_length_dict_2[book]))\n",
    "    \n",
    "       csvh.write('{}\\n'.format(','.join(row))) \n",
    "csvh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Figures 32-43, the diffusion plots\n",
    "Input: LexiconHurvitzDataset.csv\n",
    "Output: relationships_between_books_early_lat.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chron =['1chr', '2chr']\n",
    "late_books = ['ezek', 'hag', 'zech', 'mal', 'song', 'qoh', 'esth', 'dan', 'ezra', 'neh', '1chr', '2chr']\n",
    "\n",
    "#In feat_books_early and feat_books_late we collect the books in which specific variants occur. Next, in early_diff \n",
    "#and late_diff we count in how many books the features from a specific book occur\n",
    "feat_books_early = collections.defaultdict(list)\n",
    "feat_books_late = collections.defaultdict(list)\n",
    "early_diff = collections.defaultdict(lambda:collections.Counter())\n",
    "late_diff = collections.defaultdict(lambda:collections.Counter())\n",
    "\n",
    "with open('LexiconHurvitzDataset.csv', 'rt') as f:    \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if not row[1] == '7':\n",
    "            if row[4] in late_books:\n",
    "                if row[3] == '1':\n",
    "                    if row[4] in chron:\n",
    "                        if not 'chr' in feat_books_late[row[1]]:\n",
    "                            feat_books_late[row[1]].append('chr')\n",
    "                    else:\n",
    "                        if not row[4] in feat_books_late[row[1]]:    \n",
    "                            feat_books_late[row[1]].append(row[4])\n",
    "                elif row[3] == '0':\n",
    "                    if row[4] in chron:\n",
    "                        if not 'chr' in feat_books_early[row[1]]:\n",
    "                            feat_books_early[row[1]].append('chr')\n",
    "                    else:\n",
    "                        if not row[4] in feat_books_early[row[1]]:    \n",
    "                            feat_books_early[row[1]].append(row[4])\n",
    "                    \n",
    "for key in feat_books_late.keys():\n",
    "    late_diff['all_tbh_lbh'][len(feat_books_late[key])] += 1\n",
    "    for book in feat_books_late[key]:\n",
    "        late_diff[book][len(feat_books_late[key])] += 1\n",
    "for key in feat_books_early.keys():\n",
    "    early_diff['all_tbh_lbh'][len(feat_books_early[key])] += 1\n",
    "    for book in feat_books_early[key]:\n",
    "        early_diff[book][len(feat_books_early[key])] += 1\n",
    "\n",
    "#The information is storedin a csv\n",
    "csvh = open(r\"relationships_between_books_early_lat.csv \", \"w\")\n",
    "row = ['book', 'early_late', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11']\n",
    "csvh.write('{}\\n'.format(','.join(row)))\n",
    "for book in early_diff.keys():\n",
    "    row = []\n",
    "    row.append(book)\n",
    "    row.append('early')\n",
    "    for number in range(11):\n",
    "        if number + 1 in early_diff[book]:\n",
    "            row.append(str(early_diff[book][number + 1]))\n",
    "        else:\n",
    "            row.append('0')\n",
    "    csvh.write('{}\\n'.format(','.join(row)))\n",
    "for book in early_diff.keys():\n",
    "    row = []\n",
    "    row.append(book)\n",
    "    row.append('late')\n",
    "    for number in range(11):\n",
    "        if number + 1 in late_diff[book]:\n",
    "            row.append(str(late_diff[book][number + 1]))\n",
    "        else:\n",
    "            row.append('0')\n",
    "    csvh.write('{}\\n'.format(','.join(row)))    \n",
    "\n",
    "csvh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Entropy figures, Figures 45 and 46. This script uses early_freqs and late_freqs from a previous script.\n",
    "Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "late_books = ['ezek', 'hag', 'zech', 'mal', 'song', 'qoh', 'esth', 'dan', 'ezra', 'neh', 'chr']\n",
    "\n",
    "#In these dictionaries the entropy data are collected, per type and per token. \n",
    "entropy_dict = collections.defaultdict(lambda:collections.defaultdict(list))\n",
    "entropy_dict_per_type = {}\n",
    "entropy_dict_per_token = {}\n",
    "for book in late_books:\n",
    "    for key in list(set(list(early_freqs[book].keys()) + list(late_freqs[book].keys()))):\n",
    "        if key in early_freqs[book].keys():\n",
    "            entropy_dict[book][key].append(early_freqs[book][str(key)])\n",
    "            \n",
    "        else:\n",
    "            entropy_dict[book][key].append(0)\n",
    "        if key in late_freqs[book].keys():\n",
    "            entropy_dict[book][key].append(late_freqs[book][str(key)])\n",
    "        else:\n",
    "            entropy_dict[book][key].append(0)\n",
    "            \n",
    "for book in late_books:\n",
    "    total = 0\n",
    "    for key in entropy_dict[book]:\n",
    "        chance = (float(entropy_dict[book][key][1]) / (int(entropy_dict[book][key][0]) + int(entropy_dict[book][key][1])))\n",
    "        if float(chance) != 0.0:\n",
    "            if float(chance) != 1.0:\n",
    "                total += -chance * math.log(chance,2) - (1 - chance) * math.log((1 - chance), 2)\n",
    "    entropy_dict_per_type[book] = (total) / len(entropy_dict[book])\n",
    "\n",
    "for book in late_books:\n",
    "    total = 0\n",
    "    total_sum_early_late = 0\n",
    "    for key in entropy_dict[book]:\n",
    "        chance = (float(entropy_dict[book][key][1]) / (int(entropy_dict[book][key][0]) + int(entropy_dict[book][key][1])))\n",
    "        sum_early_late = entropy_dict[book][key][0] + entropy_dict[book][key][1]\n",
    "        total_sum_early_late += sum_early_late\n",
    "        #print(chance)\n",
    "        if float(chance) != 0.0:\n",
    "            if float(chance) != 1.0:\n",
    "                total += (-chance * math.log(chance,2) - (1 - chance) * math.log((1 - chance), 2)) * sum_early_late\n",
    "    entropy_dict_per_token[book] = total / total_sum_early_late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "late_books = ['ezek', 'hag', 'zech', 'mal', 'song', 'qoh', 'esth', 'dan', 'ezra', 'neh', 'chr']\n",
    "\n",
    "#similar to the previous script, but now the rate of replacement is calculated\n",
    "replacement_dict = collections.defaultdict(lambda:collections.defaultdict(list))\n",
    "replacement_dict_per_type = {}\n",
    "replacement_dict_per_token = {}\n",
    "for book in late_books:\n",
    "    for key in list(set(list(early_freqs[book].keys()) + list(late_freqs[book].keys()))):\n",
    "        if key in early_freqs[book].keys():\n",
    "            replacement_dict[book][key].append(early_freqs[book][str(key)])\n",
    "        else:\n",
    "            replacement_dict[book][key].append(0)\n",
    "        if key in late_freqs[book].keys():\n",
    "            replacement_dict[book][key].append(late_freqs[book][str(key)])\n",
    "        else:\n",
    "            replacement_dict[book][key].append(0)\n",
    "            \n",
    "for book in late_books:\n",
    "    total = 0\n",
    "    for key in replacement_dict[book]:\n",
    "        chance = (float(replacement_dict[book][key][1]) / (int(replacement_dict[book][key][0]) + int(replacement_dict[book][key][1])))\n",
    "        if chance != 0.0:\n",
    "            total += chance \n",
    "    replacement_dict_per_type[book] = (total) / len(replacement_dict[book])\n",
    "\n",
    "for book in late_books:\n",
    "    total = 0\n",
    "    total_sum_early_late = 0\n",
    "    for key in replacement_dict[book]:\n",
    "        chance = (float(replacement_dict[book][key][1]) / (int(replacement_dict[book][key][0]) + int(replacement_dict[book][key][1])))\n",
    "        sum_early_late = replacement_dict[book][key][0] + replacement_dict[book][key][1]\n",
    "        total_sum_early_late += sum_early_late\n",
    "        if chance != 0.0:\n",
    "            total += chance * sum_early_late\n",
    "    replacement_dict_per_token[book] = total / total_sum_early_late\n",
    "\n",
    "#makes a csv to be able to compare entropy and replacement\n",
    "csvh = open(r\"entropy_replacement.csv\", \"w\")\n",
    "row = ['book','entropy_type','replacement_type','entropy_token','replacement_token']\n",
    "csvh.write('{}\\n'.format(','.join(row)))\n",
    "\n",
    "for book in late_books:\n",
    "    row = []\n",
    "    row.append(book)\n",
    "    row.append(str(entropy_dict_per_type[book]))\n",
    "    row.append(str(replacement_dict_per_type[book]))\n",
    "    row.append(str(entropy_dict_per_token[book]))\n",
    "    row.append(str(replacement_dict_per_token[book]))\n",
    "    csvh.write('{}\\n'.format(','.join(row)))\n",
    "csvh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 51, Frequency of all variables in all TBH/LBH books together\n",
    "Input: LexiconHurvitzDataset.csv\n",
    "Output: freq_all_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chr = ['1chr', '2chr']\n",
    "late_books = ['ezek', 'hag', 'zech', 'mal', 'song', 'qoh', 'esth', 'dan', 'ezra', 'neh', 'chr', '1chr', '2chr']\n",
    "\n",
    "general_dict = collections.defaultdict(lambda: collections.Counter())\n",
    "\n",
    "with open('LexiconHurvitzDataset.csv', 'rt') as f:    \n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        #exclude feature 7\n",
    "        if not row[1] == '7':\n",
    "            if row[4] in late_books:\n",
    "                general_dict[row[3]][row[1]] += int(row[7])\n",
    "    \n",
    "csvh = open(r\"freq_all_features.csv\", \"w\")\n",
    "row = ['feature', 'early_late', 'frequency']\n",
    "csvh.write('{}\\n'.format(','.join(row)))\n",
    "\n",
    "for element in range(80):\n",
    "    row = []\n",
    "    row.append(str(element + 1))\n",
    "    row.append('1')\n",
    "    if not element + 1 == 7:\n",
    "        if str(element + 1) in general_dict['1'].keys():\n",
    "            row.append(str(general_dict['1'][str(element + 1)]))\n",
    "        else:\n",
    "            row.append('0')\n",
    "        csvh.write('{}\\n'.format(','.join(row)))\n",
    "        row = []\n",
    "        row.append(str(element + 1))\n",
    "        row.append('0')\n",
    "        if str(element + 1) in general_dict['0'].keys():\n",
    "            row.append(str(general_dict['0'][str(element + 1)]))\n",
    "        else:\n",
    "            row.append('0')\n",
    "        csvh.write('{}\\n'.format(','.join(row)))    \n",
    "    \n",
    "csvh.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
